import os
import json
import sqlite3
import pickle
import csv
import re
import unicodedata
from typing import Optional
from datetime import datetime
from uuid import uuid4
from functools import lru_cache

from openai import OpenAI
import numpy as np

# =========================
# 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# =========================

REPO_ROOT = os.path.dirname(os.path.abspath(__file__))


def resolve_path(env_name: str, default_relative: str) -> str:
    """
    Pick a path from environment if provided, otherwise fall back to repo-relative.
    This keeps local dev (Windows) and deploy (Linux) in sync without hardcoding drives.
    """
    env_value = os.getenv(env_name)
    if env_value:
        return env_value
    return os.path.join(REPO_ROOT, default_relative)


BASE_DIR = resolve_path("CHATBOT_DATA_DIR", "central_data")

SQLITE_PATH = os.path.join(BASE_DIR, "sqlite", "paintings.db")
# Semantic index data (fallback when keyword search returns nothing)
TOPIC_META_PATH = os.path.join(BASE_DIR, "vectors", "meta.pkl")
TOPIC_VECTORS_PATH = os.path.join(BASE_DIR, "vectors", "vectors.npy")

LOG_DIR = resolve_path("CHATBOT_LOG_DIR", "logs")
IMAGE_BASE_URL = os.getenv(
    "IMAGE_BASE_URL",
    "https://painting-cgi.s3.ap-southeast-1.amazonaws.com/",
)

# =========================
# 2. OPENAI CLIENT & API KEY
# =========================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError(
        "‚ùå Ch∆∞a thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng OPENAI_API_KEY. "
        "H√£y set trong h·ªá th·ªëng tr∆∞·ªõc khi ch·∫°y chatbot."
    )

# client v·ªõi timeout/retry ng·∫Øn h∆°n (gi·ªëng file c≈© ƒë√£ t·ªëi ∆∞u)
client = OpenAI(timeout=25, max_retries=2)

EMBED_MODEL = "text-embedding-3-small"  # 1536 chi·ªÅu
CHAT_MODEL = "gpt-4o-mini"

# =========================
# 3. TOPIC INDEX & EMBEDDING
# =========================

TOPIC_VECTORS = None
TOPIC_META = None


def load_topic_index():
    """N·∫°p topic index (d√πng cho semantic topic search)."""
    global TOPIC_VECTORS, TOPIC_META
    if TOPIC_VECTORS is not None and TOPIC_META is not None:
        return

    if not (os.path.exists(TOPIC_META_PATH) and os.path.exists(TOPIC_VECTORS_PATH)):
        TOPIC_VECTORS = None
        TOPIC_META = []
        return

    with open(TOPIC_META_PATH, "rb") as f:
        TOPIC_META = pickle.load(f)

    TOPIC_VECTORS = np.load(TOPIC_VECTORS_PATH).astype("float32")
    # Topic index loaded; keep silent to avoid noisy CLI startup.


@lru_cache(maxsize=256)
def embed_text(text: str):
    """T·∫°o embedding cho text (cache ƒë·ªÉ gi·∫£m s·ªë l·∫ßn g·ªçi API)."""
    resp = client.embeddings.create(
        model=EMBED_MODEL,
        input=text
    )
    return np.array(resp.data[0].embedding, dtype="float32")


def get_db_connection():
    if not os.path.exists(SQLITE_PATH):
        raise RuntimeError(
            f"SQLite database not found at {SQLITE_PATH}. "
            "Set CHATBOT_DATA_DIR to point to the data directory."
        )
    conn = sqlite3.connect(SQLITE_PATH)
    conn.row_factory = sqlite3.Row
    return conn


def normalize_query_for_like(q: str) -> str:
    return f"%{q.strip()}%"


def build_image_url(img_path: str) -> str:
    """Normalize relative image path to absolute URL."""
    if not img_path:
        return ""
    clean = img_path.replace("\\", "/").lstrip("/")
    if clean.startswith("cgi/"):
        clean = clean[len("cgi/"):]
    return f"{IMAGE_BASE_URL.rstrip('/')}/{clean}"


# =========================
# 4. AGENT: RETRIEVER
# =========================

STOPWORDS = {"tranh", "buc", "con", "hinh", "anh", "ve"}


def strip_accents(text: str) -> str:
    """Remove accents for accent-insensitive comparisons."""
    normalized = unicodedata.normalize("NFD", text or "")
    return "".join(ch for ch in normalized if unicodedata.category(ch) != "Mn")


def extract_tokens(query: str):
    """
    Normalize and split the user query into tokens:
    - lowercase + strip accents
    - split on non-alphanumeric
    - drop stopwords like 'tranh', 'con' to focus on core nouns
    """
    clean = strip_accents((query or "").lower())
    raw_tokens = re.split(r"[^a-z0-9]+", clean)
    return [t for t in raw_tokens if t and t not in STOPWORDS]


class RetrieverAgent:
    """
    Nhi·ªám v·ª•:
    - T√¨m tranh trong SQLite b·∫±ng keyword.
    - N·∫øu c·∫ßn th√¨ d√πng semantic topic search (theo topic_meta + vectors).
    """

    def keyword_search_paintings(self, user_input: str, limit: Optional[int] = None):
        conn = get_db_connection()
        conn.create_function("strip_accents", 1, strip_accents)
        cur = conn.cursor()

        tokens = extract_tokens(user_input)
        if not tokens:
            normalized = strip_accents((user_input or "").lower().strip())
            tokens = [normalized] if normalized else []

        columns = ["title", "keywords", "themes", "emotions"]
        clauses = []
        params = []
        for tok in tokens:
            like_pattern = f"%{tok}%"
            clause = " OR ".join([f"strip_accents(lower({col})) LIKE ?" for col in columns])
            clauses.append(f"({clause})")
            params.extend([like_pattern] * len(columns))

        where_sql = " AND ".join(clauses) if clauses else "1=1"

        query = f"""
            SELECT id, title, image, general_info, keywords, themes, emotions,
                   description_short, json_path
            FROM paintings
            WHERE {where_sql}
            ORDER BY id ASC
        """
        if limit is not None:
            query += "\n            LIMIT ?;"
            params.append(limit)
        else:
            query += ";"

        cur.execute(query, params)
        rows = cur.fetchall()
        conn.close()

        results = []
        for r in rows:
            results.append({
                "id": r["id"],
                "title": r["title"],
                "image": r["image"],
                "general_info": r["general_info"],
                "keywords": (r["keywords"] or "").split(",") if r["keywords"] else [],
                "themes": (r["themes"] or "").split(",") if r["themes"] else [],
                "emotions": (r["emotions"] or "").split(",") if r["emotions"] else [],
                "description_short": r["description_short"],
                "json_path": r["json_path"]
            })

        return results

    def semantic_topic_search(
        self,
        user_input: str,
        top_k_topics: int = 2,
        max_items: Optional[int] = None
    ):
        load_topic_index()
        if TOPIC_VECTORS is None or len(TOPIC_META) == 0:
            return []

        try:
            q_vec = embed_text(user_input)
        except Exception as e:
            print(f"[Retriever] Semantic embedding error: {e}")
            return []
        q_norm = q_vec / (np.linalg.norm(q_vec) + 1e-8)
        topic_norm = TOPIC_VECTORS / (
            np.linalg.norm(TOPIC_VECTORS, axis=1, keepdims=True) + 1e-8
        )

        scores = topic_norm @ q_norm  # cosine similarity
        top_idx = np.argsort(scores)[::-1][:top_k_topics]

        candidate_ids = []
        for idx in top_idx:
            meta = TOPIC_META[idx]
            ids = meta.get("suggest_ids") or []
            if not ids and meta.get("id") is not None:
                ids = [meta.get("id")]
            candidate_ids.extend(ids)

        # lo·∫°i tr√πng, gi·ªØ th·ª© t·ª±
        seen = set()
        unique_ids = []
        max_allowed = max_items if max_items is not None else float("inf")
        for i in candidate_ids:
            if i not in seen:
                seen.add(i)
                unique_ids.append(i)
            if len(unique_ids) >= max_allowed:
                break

        if not unique_ids:
            return []

        conn = get_db_connection()
        cur = conn.cursor()

        placeholders = ",".join("?" for _ in unique_ids)
        sql = f"""
            SELECT id, title, image, general_info, keywords, themes, emotions,
                   description_short, json_path
            FROM paintings
            WHERE id IN ({placeholders});
        """
        cur.execute(sql, unique_ids)
        rows = cur.fetchall()
        conn.close()

        row_map = {r["id"]: r for r in rows}
        results = []
        for pid in unique_ids:
            r = row_map.get(pid)
            if not r:
                continue
            results.append({
                "id": r["id"],
                "title": r["title"],
                "image": r["image"],
                "general_info": r["general_info"],
                "keywords": (r["keywords"] or "").split(",") if r["keywords"] else [],
                "themes": (r["themes"] or "").split(",") if r["themes"] else [],
                "emotions": (r["emotions"] or "").split(",") if r["emotions"] else [],
                "description_short": r["description_short"],
                "json_path": r["json_path"]
            })

        return results

    def search_paintings_for_user_query(self, user_input: str, max_results: Optional[int] = None):
        """
        Router search:
        - ∆Øu ti√™n keyword.
        - N·∫øu keyword tr·∫£ qu√° √≠t k·∫øt qu·∫£ -> d√πng th√™m semantic topic search.
        """
        kw_results = self.keyword_search_paintings(user_input, limit=max_results)
        if kw_results:
            print(f"[Retriever] Keyword search tr·∫£ {len(kw_results)} k·∫øt qu·∫£, d√πng tr·ª±c ti·∫øp.")
            return kw_results

        print("[Retriever] Keyword √≠t k·∫øt qu·∫£ -> th√™m semantic topic search.")
        sem_results = self.semantic_topic_search(
            user_input, top_k_topics=2, max_items=max_results
        )

        if sem_results:
            print(f"[Retriever] Semantic topic search tr·∫£ {len(sem_results)} k·∫øt qu·∫£.")
            return sem_results

        print("[Retriever] Kh√¥ng c√≥ semantic -> fallback keyword.")
        return kw_results


# =========================
# 5. AGENT: SUMMARIZER
# =========================

class SummarizerAgent:
    """
    Nhi·ªám v·ª•:
    - Vi·∫øt 1 ƒëo·∫°n gi·ªõi thi·ªáu ng·∫Øn (2‚Äì4 c√¢u) v·ªÅ ch·ªß ƒë·ªÅ tranh ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa kh√°ch.
    - Kh√¥ng render gallery, ch·ªâ text gi·ªõi thi·ªáu.
    """

    SYSTEM_PROMPT = """
B·∫°n l√† nh√¢n vi√™n t∆∞ v·∫•n b√°n tranh.
Nhi·ªám v·ª•: Vi·∫øt ƒëo·∫°n gi·ªõi thi·ªáu NG·∫ÆN G·ªåN (2‚Äì4 c√¢u) v·ªÅ b·ªô s∆∞u t·∫≠p tranh
ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa kh√°ch h√†ng.

Quy t·∫Øc:
- Kh√¥ng li·ªát k√™ t·ª´ng b·ª©c tranh chi ti·∫øt.
- Ch·ªâ n√≥i kh√°i qu√°t v·ªÅ phong c√°ch, c·∫£m x√∫c, kh√¥ng gian ph√π h·ª£p.
- Gi·ªçng vƒÉn th√¢n thi·ªán, r√µ r√†ng, ∆∞u ti√™n s√∫c t√≠ch.
"""

    def _compact_for_summary(self, products, max_items: int = 10):
        """Gi·ªØ th√¥ng tin t·ªëi thi·ªÉu cho Summarizer ƒë·ªÉ gi·∫£m token."""
        trimmed = []
        for item in products[:max_items]:
            if not isinstance(item, dict):
                continue
            trimmed.append({
                "id": item.get("id"),
                "title": item.get("title"),
                "general_info": item.get("general_info"),
                "themes": item.get("themes"),
                "emotions": item.get("emotions"),
                "description_short": item.get("description_short"),
            })
        return trimmed

    def summarize(self, user_input: str, products: list) -> str:
        if not products:
            return "Hi·ªán t·∫°i m√¨nh ch∆∞a t√¨m th·∫•y b·ª©c tranh ph√π h·ª£p trong kho d·ªØ li·ªáu."

        compacted = self._compact_for_summary(products)
        context_text = json.dumps(compacted, ensure_ascii=False, separators=(",", ":"))

        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {
                "role": "user",
                "content": (
                    f"Y√™u c·∫ßu c·ªßa kh√°ch: {user_input}\n\n"
                    f"D·ªØ li·ªáu t√≥m t·∫Øt c√°c tranh:\n{context_text}"
                )
            },
        ]

        resp = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=messages,
            temperature=0.7,
            max_tokens=400,
            response_format={"type": "text"},
        )

        return resp.choices[0].message.content


# =========================
# 6. AGENT: DESIGNER (RENDER UI/HTML)
# =========================

class DesignerAgent:
    """
    Nhi·ªám v·ª•:
    - Enrich d·ªØ li·ªáu (th√™m image_html, link_html).
    - Render HTML layout: ph·∫ßn tr√™n l√† intro_text, ph·∫ßn d∆∞·ªõi l√† gallery nhi·ªÅu tranh.
    """

    @staticmethod
    def enrich_product_data(context_list):
        for item in context_list:
            if isinstance(item, dict) and "image" in item and "id" in item:
                img_path = item["image"]  # vd: "cgi/28.jpg"
                sp_id = item["id"]        # vd: 28
                image_url = build_image_url(img_path)

                image_html = (
                    f"<img src='{image_url}' "
                    f"style='max-width: 100%; border-radius: 10px;'>"
                )
                link_html = (
                    f"<a class='link-btn' href='https://cgi.vn/san-pham/{sp_id}' "
                    f"target='_blank'>Xem chi ti·∫øt</a>"
                )
                ar_link_html = (
                    f"<a class='link-btn' href='https://cgi.vn/ar/{sp_id}.html' "
                    f"target='_blank'>Xem AR</a>"
                )

                item["image_html"] = image_html
                item["h√¨nh_html"] = image_html  # alias
                item["link_html"] = link_html
                item["ar_link_html"] = ar_link_html

        return context_list

    def render_gallery(self, intro_text: str, products: list) -> str:
        """
        T·∫°o HTML ho√†n ch·ªânh:
        - Ph·∫ßn ƒë·∫ßu: intro_text.
        - Ph·∫ßn sau: gallery t·∫•t c·∫£ tranh.
        """
        if not products:
            return (
                "<p>Hi·ªán t·∫°i m√¨nh ch∆∞a t√¨m th·∫•y b·ª©c tranh ph√π h·ª£p trong kho d·ªØ li·ªáu.</p>"
            )

        products = self.enrich_product_data(products)

        html_parts = []
        # Ph·∫ßn gi·ªõi thi·ªáu ng·∫Øn
        if intro_text:
            html_parts.append(f"<p>{intro_text}</p>")

        # Ph·∫ßn gallery
        html_parts.append("<h3>Danh s√°ch tranh g·ª£i √Ω</h3>")
        html_parts.append('<div class="gallery">')

        for item in products:
            title = item.get("title") or "Tranh"
            image_html = (
                item.get("image_html")
                or item.get("h√¨nh_html")
                or "<div>(Kh√¥ng c√≥ h√¨nh)</div>"
            )
            link_html = item.get("link_html") or ""
            ar_link_html = item.get("ar_link_html") or ""
            links_combined = ""
            if ar_link_html or link_html:
                separator = "<span class=\"link-separator\">|</span>" if ar_link_html and link_html else ""
                links_combined = f"<p class='links-row'>{ar_link_html}{separator}{link_html}</p>"

            block = f"""
            <div class="item" style="margin-bottom: 16px;">
                <h4>{title}</h4>
                {image_html}
                {links_combined}
            </div>
            """
            html_parts.append(block)

        html_parts.append("</div>")
        return "\n".join(html_parts)


# =========================
# 7. AGENT: LOGS
# =========================

SESSION_ID = str(uuid4())[:8]


class LogAgent:
    """
    Nhi·ªám v·ª•:
    - Ghi log d·∫°ng CSV: m·ªói ng√†y 1 file, m·ªói d√≤ng l√† 1 l∆∞·ª£t chat.
    """

    def __init__(self, log_dir: str = LOG_DIR):
        self.log_dir = log_dir
        os.makedirs(self.log_dir, exist_ok=True)

    def log_chat(self, user_input: str, bot_reply: str):
        today = datetime.now().strftime("%Y-%m-%d")
        log_file = os.path.join(self.log_dir, f"{today}.csv")

        current_time = datetime.now().strftime("%H:%M:%S")
        is_new = not os.path.exists(log_file)

        with open(log_file, "a", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            if is_new:
                writer.writerow(["ID phi√™n", "Th·ªùi gian", "Ng∆∞·ªùi d√πng", "Chatbot"])
            writer.writerow([SESSION_ID, current_time, user_input, bot_reply])


# =========================
# 8. AGENT: DIRECTOR (ƒêI·ªÄU PH·ªêI)
# =========================

class DirectorAgent:
    """
    Nhi·ªám v·ª•:
    - Nh·∫≠n c√¢u h·ªèi t·ª´ user.
    - G·ªçi Retriever -> l·∫•y danh s√°ch tranh.
    - G·ªçi Summarizer -> t·∫°o intro ng·∫Øn.
    - G·ªçi Designer -> render HTML tr·∫£ v·ªÅ.
    - G·ªçi LogAgent -> ghi log.
    """

    def __init__(self):
        self.retriever = RetrieverAgent()
        self.summarizer = SummarizerAgent()
        self.designer = DesignerAgent()
        self.logger = LogAgent()

    def handle_user_message(self, user_input: str) -> str:
        # 1. L·∫•y d·ªØ li·ªáu tranh
        products = self.retriever.search_paintings_for_user_query(user_input)

        # 2. T·∫°o ƒëo·∫°n gi·ªõi thi·ªáu ng·∫Øn
        intro_text = self.summarizer.summarize(user_input, products)

        # 3. Render HTML layout
        response_html = self.designer.render_gallery(intro_text, products)

        # 4. Ghi log
        self.logger.log_chat(user_input, response_html)

        return response_html


# =========================
# 9. MAIN CHATBOT LOOP (CLI)
# =========================

def chatbot_cli():
    # Preload topic index ƒë·ªÉ tr√°nh ƒë·ªçc file ·ªü request ƒë·∫ßu ti√™n
    load_topic_index()

    director = DirectorAgent()

    print("ü§ñ Chatbot (multi-agent) ƒë√£ s·∫µn s√†ng! G√µ 'exit' ƒë·ªÉ tho√°t.\n")
    while True:
        user_input = input("B·∫°n: ")
        if user_input.lower().strip() == "exit":
            print("üëã Chatbot k·∫øt th√∫c.")
            break

        try:
            reply = director.handle_user_message(user_input)
        except Exception as e:
            print("‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω:", e)
            reply = "Xin l·ªói, hi·ªán t·∫°i m√¨nh ƒëang g·∫∑p ch√∫t tr·ª•c tr·∫∑c h·ªá th·ªëng, b·∫°n th·ª≠ l·∫°i sau nh√©."

        print("Chatbot (HTML):")
        print(reply)
        print("-" * 40)


if __name__ == "__main__":
    chatbot_cli()
