import os
import json
import sqlite3
import pickle
from datetime import datetime
from openpyxl import Workbook, load_workbook
from uuid import uuid4

from openai import OpenAI
import numpy as np

# =========================
# 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# =========================

BASE_DIR = r"F:\Chatbot\central_data"

SQLITE_PATH = os.path.join(BASE_DIR, "sqlite", "paintings.db")
TOPIC_META_PATH = os.path.join(BASE_DIR, "topics", "topic_meta.pkl")
TOPIC_VECTORS_PATH = os.path.join(BASE_DIR, "topics", "topic_vectors.npy")

LOG_DIR = r"F:\Chatbot\logs"

# =========================
# 2. OPENAI CLIENT & API KEY
# =========================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError(
        "‚ùå Ch∆∞a thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng OPENAI_API_KEY. "
        "H√£y set trong h·ªá th·ªëng tr∆∞·ªõc khi ch·∫°y chatbot."
    )

client = OpenAI()  # t·ª± ƒë·ªçc OPENAI_API_KEY t·ª´ env

EMBED_MODEL = "text-embedding-3-small"  # 1536 chi·ªÅu
CHAT_MODEL = "gpt-4o-mini"

# =========================
# 3. LOAD TOPIC INDEX
# =========================

TOPIC_VECTORS = None
TOPIC_META = None

def load_topic_index():
    global TOPIC_VECTORS, TOPIC_META
    if TOPIC_VECTORS is not None and TOPIC_META is not None:
        return

    if not (os.path.exists(TOPIC_META_PATH) and os.path.exists(TOPIC_VECTORS_PATH)):
        print("‚ö† Kh√¥ng t√¨m th·∫•y topic index, s·∫Ω b·ªè qua semantic topic search.")
        TOPIC_VECTORS = None
        TOPIC_META = []
        return

    with open(TOPIC_META_PATH, "rb") as f:
        TOPIC_META = pickle.load(f)

    TOPIC_VECTORS = np.load(TOPIC_VECTORS_PATH).astype("float32")
    print(f"‚úÖ ƒê√£ n·∫°p topic index: {TOPIC_VECTORS.shape[0]} topic")

# =========================
# 4. LOG CHAT V√ÄO EXCEL
# =========================

SESSION_ID = str(uuid4())[:8]

def log_chat(user_input, bot_reply):
    os.makedirs(LOG_DIR, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")
    log_file = os.path.join(LOG_DIR, f"{today}.xlsx")

    if os.path.exists(log_file):
        wb = load_workbook(log_file)
        ws = wb.active
    else:
        wb = Workbook()
        ws = wb.active
        ws.append(["ID phi√™n", "Th·ªùi gian", "Ng∆∞·ªùi d√πng", "Chatbot"])

    current_time = datetime.now().strftime("%H:%M:%S")
    ws.append([SESSION_ID, current_time, user_input, bot_reply])
    wb.save(log_file)

# =========================
# 5. H√ÄM TI·ªÜN √çCH
# =========================

def get_db_connection():
    conn = sqlite3.connect(SQLITE_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def embed_text(text: str):
    resp = client.embeddings.create(
        model=EMBED_MODEL,
        input=text
    )
    return np.array(resp.data[0].embedding, dtype="float32")

def normalize_query_for_like(q: str) -> str:
    return f"%{q.strip()}%"

# =========================
# 6. SEARCH SQLITE THEO KEYWORD
# =========================

def keyword_search_paintings(user_input: str, limit: int = 20):
    conn = get_db_connection()
    cur = conn.cursor()

    like_pattern = normalize_query_for_like(user_input)

    query = """
        SELECT id, title, image, general_info, keywords, themes, emotions,
               description_short, json_path
        FROM paintings
        WHERE 
            title LIKE ? OR
            keywords LIKE ? OR
            themes LIKE ? OR
            emotions LIKE ?
        LIMIT ?;
    """

    cur.execute(query, (like_pattern, like_pattern, like_pattern, like_pattern, limit))
    rows = cur.fetchall()
    conn.close()

    results = []
    for r in rows:
        results.append({
            "id": r["id"],
            "title": r["title"],
            "image": r["image"],
            "general_info": r["general_info"],
            "keywords": (r["keywords"] or "").split(",") if r["keywords"] else [],
            "themes": (r["themes"] or "").split(",") if r["themes"] else [],
            "emotions": (r["emotions"] or "").split(",") if r["emotions"] else [],
            "description_short": r["description_short"],
            "json_path": r["json_path"]
        })

    return results

# =========================
# 7. SEMANTIC TOPIC SEARCH
# =========================

def semantic_topic_search(user_input: str, top_k_topics: int = 3, max_items: int = 20):
    load_topic_index()
    if TOPIC_VECTORS is None or len(TOPIC_META) == 0:
        return []

    q_vec = embed_text(user_input)
    # chu·∫©n h√≥a
    q_norm = q_vec / (np.linalg.norm(q_vec) + 1e-8)
    topic_norm = TOPIC_VECTORS / (np.linalg.norm(TOPIC_VECTORS, axis=1, keepdims=True) + 1e-8)

    scores = topic_norm @ q_norm  # cosine similarity
    top_idx = np.argsort(scores)[::-1][:top_k_topics]

    # gom id tranh t·ª´ c√°c topic g·∫ßn nh·∫•t
    candidate_ids = []
    for idx in top_idx:
        meta = TOPIC_META[idx]
        ids = meta.get("suggest_ids", [])
        candidate_ids.extend(ids)

    # lo·∫°i tr√πng, gi·ªØ th·ª© t·ª±
    seen = set()
    unique_ids = []
    for i in candidate_ids:
        if i not in seen:
            seen.add(i)
            unique_ids.append(i)
        if len(unique_ids) >= max_items:
            break

    if not unique_ids:
        return []

    # l·∫•y chi ti·∫øt tranh t·ª´ SQLite
    conn = get_db_connection()
    cur = conn.cursor()

    placeholders = ",".join("?" for _ in unique_ids)
    sql = f"""
        SELECT id, title, image, general_info, keywords, themes, emotions,
               description_short, json_path
        FROM paintings
        WHERE id IN ({placeholders});
    """
    cur.execute(sql, unique_ids)
    rows = cur.fetchall()
    conn.close()

    # map theo th·ª© t·ª± unique_ids
    row_map = {r["id"]: r for r in rows}
    results = []
    for pid in unique_ids:
        r = row_map.get(pid)
        if not r:
            continue
        results.append({
            "id": r["id"],
            "title": r["title"],
            "image": r["image"],
            "general_info": r["general_info"],
            "keywords": (r["keywords"] or "").split(",") if r["keywords"] else [],
            "themes": (r["themes"] or "").split(",") if r["themes"] else [],
            "emotions": (r["emotions"] or "").split(",") if r["emotions"] else [],
            "description_short": r["description_short"],
            "json_path": r["json_path"]
        })

    return results

# =========================
# 8. ENRICH DATA (TH√äM HTML H√åNH & LINK)
# =========================

def enrich_product_data(context_list):
    for item in context_list:
        if isinstance(item, dict) and "image" in item and "id" in item:
            img_path = item["image"]  # vd: "cgi/28.jpg"
            sp_id = item["id"]        # vd: 28

            item["h√¨nh_html"] = (
                f"<img src='/static/product/{img_path}' "
                f"style='max-width: 100%; border-radius: 10px;'>"
            )
            item["link_html"] = (
                f"<p><a href='https://cgi.vn/san-pham/{sp_id}' "
                f"target='_blank'>Xem chi ti·∫øt s·∫£n ph·∫©m</a></p>"
            )
    return context_list

# =========================
# 9. PROMPT & G·ªåI GPT
# =========================

SYSTEM_PROMPT = """
B·∫°n l√† m·ªôt nh√¢n vi√™n t∆∞ v·∫•n b√°n tranh.

M·ª§C TI√äU HI·ªÇN TH·ªä:
1) Ph·∫ßn tr√™n: ch·ªâ gi·ªõi thi·ªáu NG·∫ÆN G·ªåN (2‚Äì4 c√¢u) v·ªÅ ch·ªß ƒë·ªÅ tranh ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa kh√°ch.
2) Ph·∫ßn d∆∞·ªõi: HI·ªÇN TH·ªä DANH S√ÅCH NHI·ªÄU TRANH (t·∫•t c·∫£ tranh c√≥ trong d·ªØ li·ªáu ƒë·∫ßu v√†o), d·∫°ng gallery.
   - M·ªói tranh:
     - Hi·ªÉn th·ªã ti√™u ƒë·ªÅ (title)
     - Hi·ªÉn th·ªã h√¨nh HTML ƒë√£ cung c·∫•p trong tr∆∞·ªùng "h√¨nh_html"
     - Hi·ªÉn th·ªã link HTML ƒë√£ cung c·∫•p trong tr∆∞·ªùng "link_html"
   - KH√îNG vi·∫øt m√¥ t·∫£ d√†i cho t·ª´ng tranh. N·∫øu c·∫ßn, ch·ªâ 1 c√¢u r·∫•t ng·∫Øn.

QUY T·∫ÆC QUAN TR·ªåNG:
- LU√îN s·ª≠ d·ª•ng ƒë√∫ng "h√¨nh_html" v√† "link_html" c√≥ trong d·ªØ li·ªáu, KH√îNG t·ª± b·ªãa ƒë∆∞·ªùng d·∫´n.
- KH√îNG d√πng markdown ki·ªÉu ![ ](...) ho·∫∑c link gi·∫£ #.
- KH√îNG t·ª± √Ω r√∫t g·ªçn danh s√°ch tranh xu·ªëng c√≤n 3‚Äì5 b·ª©c.
  ‚Üí H√£y hi·ªÉn th·ªã ƒê·∫¶Y ƒê·ª¶ T·∫§T C·∫¢ c√°c tranh trong m·∫£ng d·ªØ li·ªáu ƒë√£ cung c·∫•p.
- C√≥ th·ªÉ tr√¨nh b√†y danh s√°ch tranh theo d·∫°ng:

  <h3>Danh s√°ch tranh g·ª£i √Ω</h3>
  <div class="gallery">
    <!-- l·∫∑p qua t·ª´ng tranh -->
    <div class="item">
      <h4>{title}</h4>
      {h√¨nh_html}
      {link_html}
    </div>
  </div>

- Gi·ªØ gi·ªçng vƒÉn th√¢n thi·ªán, d·ªÖ hi·ªÉu, nh∆∞ng ∆∞u ti√™n NG·∫ÆN G·ªåN.
"""

def query_openai_with_context(context_list, user_input):
    context_list = enrich_product_data(context_list)
    context_text = json.dumps(context_list, ensure_ascii=False, indent=2)

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"Kh√°ch h·ªèi: {user_input}\n\nD·ªØ li·ªáu s·∫£n ph·∫©m:\n{context_text}"}
    ]

    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages,
        temperature=0.7
    )

    return resp.choices[0].message.content

# =========================
# 10. ROUTER SEARCH: KEYWORD + SEMANTIC
# =========================

def search_paintings_for_user_query(user_input: str, max_results: int = 20):
    # B1: th·ª≠ keyword search tr∆∞·ªõc
    kw_results = keyword_search_paintings(user_input, limit=max_results)
    if len(kw_results) >= 8:
        print(f"üîé Keyword search tr·∫£ {len(kw_results)} k·∫øt qu·∫£ ‚Üí d√πng tr·ª±c ti·∫øp.")
        return kw_results

    # B2: n·∫øu keyword √≠t k·∫øt qu·∫£ ‚Üí d√πng semantic topic search
    print(f"üîç Keyword ch·ªâ c√≥ {len(kw_results)} k·∫øt qu·∫£ ‚Üí d√πng th√™m semantic topic search.")
    sem_results = semantic_topic_search(user_input, top_k_topics=3, max_items=max_results)

    # N·∫øu semantic c√≥ k·∫øt qu·∫£ ‚Üí ∆∞u ti√™n
    if sem_results:
        print(f"üß† Semantic topic search tr·∫£ {len(sem_results)} k·∫øt qu·∫£.")
        return sem_results

    # N·∫øu semantic c≈©ng kh√¥ng c√≥ ‚Üí fallback v·ªÅ keyword
    print("‚ö† Semantic topic search kh√¥ng c√≥ k·∫øt qu·∫£ ‚Üí fallback keyword.")
    return kw_results

# =========================
# 11. MAIN CHATBOT LOOP
# =========================

def chatbot():
    print("ü§ñ Chatbot ƒë√£ s·∫µn s√†ng! G√µ 'exit' ƒë·ªÉ tho√°t.\n")
    while True:
        user_input = input("B·∫°n: ")
        if user_input.lower().strip() == "exit":
            print("üëã Chatbot k·∫øt th√∫c.")
            break

        try:
            context_list = search_paintings_for_user_query(user_input)
            gpt_reply = query_openai_with_context(context_list, user_input)
        except Exception as e:
            print("‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω:", e)
            gpt_reply = "Xin l·ªói, hi·ªán t·∫°i m√¨nh ƒëang g·∫∑p ch√∫t tr·ª•c tr·∫∑c h·ªá th·ªëng, b·∫°n th·ª≠ l·∫°i sau nh√©."

        print("Chatbot:", gpt_reply)
        log_chat(user_input, gpt_reply)

if __name__ == "__main__":
    chatbot()
